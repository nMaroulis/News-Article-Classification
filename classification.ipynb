{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, io, os, errno, fileinput, csv\n",
    "import collections as cl\n",
    "from os import path\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "trainset_file = \"./datasets/train_set.csv\"\n",
    "train_df = pd.read_csv(trainset_file,  sep='\\t')\n",
    "EvaluationMetric_10fold_csv = \"./datasets/EvaluationMetric_10fold.csv\"\n",
    "# train_df = train_df.sample(frac=1) # SUFFLE\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(train_df['Content'])\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "EvaluationMetric_10fold = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training - Testing Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten_fold_cross_validation(model, model_name):\n",
    "    \"\"\"ten fold cross validation using the input model\"\"\"\n",
    "    \n",
    "    # scores = cross_val_score(model, train_df['Content'], train_df['Category'], cv=10, scoring='recall_macro')\n",
    "    fold_size = int (np.size(train_df,0) / 10 ) \n",
    "    #result = []\n",
    "    result = 0\n",
    "    for i in range(10):\n",
    "        \n",
    "        from_ = i*fold_size\n",
    "        to_ = (i+1)*fold_size\n",
    "        if i == 9:\n",
    "            to_ = 12266\n",
    "\n",
    "        test_X = train_df['Content'][from_:to_] # Validation Set\n",
    "        test_Y = train_df['Category'][from_:to_]\n",
    "        train_set = train_df.drop(train_df.index[from_:to_])  # train_set = np.delete(train_df, np.s_[from_:to_], 0)\n",
    "        train_X = train_set['Content']\n",
    "        train_Y = train_set['Category']\n",
    "\n",
    "        # print('Iteration',i,': ', from_, to_)\n",
    "        _ = model.fit(train_X, train_Y)\n",
    "        predicted = model.predict(test_X)\n",
    "        # result.append([model_name,'Accuracy ', np.mean(predicted == test_Y), '\\n',metrics.classification_report(test_Y, predicted, target_names=['Business','Film','Football','Politics','Technology'])])\n",
    "        result += np.mean(predicted == test_Y)\n",
    "    res = '<10fold> Accuracy Score for ' + model_name + ' is ' + str(result/10)\n",
    "    return res\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def ten_fold_cross_validation_metrics(model, model_name):\n",
    "    \"\"\"ten fold cross validation using the input model using sklearn metrics\"\"\"\n",
    "    \n",
    "    fold_size = int (np.size(train_df,0) / 10 ) \n",
    "    result = []\n",
    "    \n",
    "    acc = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1_sc = 0\n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "        from_ = i*fold_size\n",
    "        to_ = (i+1)*fold_size\n",
    "        if i == 9:\n",
    "            to_ = 12266\n",
    "\n",
    "        test_X = train_df['Content'][from_:to_] # Validation Set\n",
    "        test_Y = train_df['Category'][from_:to_]\n",
    "        train_set = train_df.drop(train_df.index[from_:to_])  # train_set = np.delete(train_df, np.s_[from_:to_], 0)\n",
    "        train_X = train_set['Content']\n",
    "        train_Y = train_set['Category']\n",
    "        _ = model.fit(train_X, train_Y)\n",
    "        predicted = model.predict(test_X)\n",
    "        \n",
    "        precision += precision_score(test_Y, predicted, average='macro')\n",
    "        recall += recall_score(test_Y, predicted, average='macro')\n",
    "        acc += np.mean(predicted == test_Y)\n",
    "        f1_sc += f1_score(test_Y, predicted, average='macro')\n",
    "        print('10fcv - Iteration',str(i+1))\n",
    "    result.append([acc/10, recall/10, precision/10, f1_sc/10])\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def simple_split(model, model_name):\n",
    "    \"\"\" 0.33 split for test data \"\"\"\n",
    "    \n",
    "    train_X = train_df['Content'][0:8177]\n",
    "    train_Y = train_df['Category'][0:8177]\n",
    "\n",
    "    test_X = train_df['Content'][8177:12266]\n",
    "    test_Y = train_df['Category'][8177:12266]\n",
    "    \n",
    "    _ = model.fit(train_X, train_Y)\n",
    "    predicted = model.predict(test_X)\n",
    "    result = np.mean(predicted == test_Y)\n",
    "    res = '<split> Accuracy Score for ' + model_name + ' is ' + str(np.mean(predicted == test_Y))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear SVM__ with __Bag of Words__ and Tf-Idf transformation on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Linear SVM with BoW: \n",
      " Accuracy:  0.9520665610898073 \n",
      " Recall:  0.948442896140975 \n",
      " Precision:  0.9498583181168814 \n",
      " F1-score:  0.9489486170952942\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "\"\"\" \n",
    "    kernel: linear\n",
    "    misclassification penalty factor C: 1.1\n",
    "    Multiple Class Approach decision function: ovo (one versus one ) || ovr (one versus rest)\n",
    "    1) linear kernel\n",
    "    2) Radial Basis Function (RBF) kernel: set gamma value (e.g. 2^-4)\n",
    "\"\"\"\n",
    "\n",
    "svm_bow = Pipeline([\n",
    "    # ('hash_vect', HashingVectorizer()),\n",
    "    # ('tfidf', TfidfTransformer()),\n",
    "    ('vect', CountVectorizer(analyzer ='word', lowercase=True)),\n",
    "    ('svm', svm.SVC(kernel='linear', C=1.0, decision_function_shape='ovr')),])\n",
    "\n",
    "\n",
    "# print(simple_split(svm_bow, 'Linear SVM with BoW'))\n",
    "\n",
    "svm_bow_res = ten_fold_cross_validation_metrics(svm_bow,'Linear SVM with BoW')\n",
    "EvaluationMetric_10fold.append([svm_bow_res[0][0], svm_bow_res[0][2], svm_bow_res[0][1], svm_bow_res[0][3]]) # append result to the list\n",
    "print(\"Linear SVM with BoW:\", \"\\n Accuracy: \", svm_bow_res[0][0], \"\\n Recall: \", svm_bow_res[0][1], \"\\n Precision: \", svm_bow_res[0][2], \"\\n F1-score: \", svm_bow_res[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest__ with __Bag of Words__ and Tf-Idf transformation on features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Random Forest with BoW: \n",
      " Accuracy:  0.9439095570009111 \n",
      " Recall:  0.9319728706772252 \n",
      " Precision:  0.9439764026613858 \n",
      " F1-score:  0.9365171085309854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\"\"\" \n",
    "    number of decision trees: 240\n",
    "    max_depth in tree: 30\n",
    "\"\"\"\n",
    "random_forest_bow = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer ='word', lowercase=True)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=240, max_depth=30, random_state=0)),])\n",
    "\n",
    "\n",
    "# print(ten_fold_cross_validation(text_random_forest, 'Random Forest with BoW'))\n",
    "\n",
    "random_forest_bow_res = ten_fold_cross_validation_metrics(random_forest_bow,'Random Forest with BoW')\n",
    "EvaluationMetric_10fold.append([random_forest_bow_res[0][0], random_forest_bow_res[0][2], random_forest_bow_res[0][1], random_forest_bow_res[0][3]])\n",
    "\n",
    "print(\"Random Forest with BoW:\", \"\\n Accuracy: \", random_forest_bow_res[0][0], \"\\n Recall: \", random_forest_bow_res[0][1], \"\\n Precision: \", random_forest_bow_res[0][2], \"\\n F1-score: \", random_forest_bow_res[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Compute SVD Output Vector Size for Variance > 90%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "def find_svd_feature_size(var_ratio):\n",
    "   \n",
    "    total_variance = 0.0  # Set initial variance explained so far\n",
    "    \n",
    "    n_components = 0 # Set initial number of features\n",
    "    \n",
    "    # For the explained variance of each feature:\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        total_variance += explained_variance # Add the explained variance to the total\n",
    "        n_components += 1  # Add one to the number of components\n",
    "\n",
    "        # If variance >= 90%\n",
    "        if total_variance >= 0.9:\n",
    "            break\n",
    "            \n",
    "    return n_components\n",
    "\n",
    "\"\"\"\n",
    "Start by using SVD on the term vector (size 85747), with m_components 2000 (too many),\n",
    "in order to find the n_components value in order to have variance >= 90%\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# tsvd = TruncatedSVD(n_components=2000)\n",
    "# X_tsvd = tsvd.fit(X_train_counts)\n",
    "\n",
    "# tsvd_var_ratios = tsvd.explained_variance_ratio_ # List of explained variances\n",
    "# component_num = find_svd_feature_size(tsvd_var_ratios)\n",
    "\n",
    "component_num = 593 # TEMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear SVM__ using __SVD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Linear SVM with SVD: \n",
      " Accuracy:  0.9429311614160717 \n",
      " Recall:  0.9376118460559901 \n",
      " Precision:  0.9408134414645571 \n",
      " F1-score:  0.9390072441882031\n"
     ]
    }
   ],
   "source": [
    "svm_svd = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word')),\n",
    "    ('svd', TruncatedSVD(n_components=component_num)),\n",
    "    ('svm', svm.SVC(kernel='linear', C=1.0, decision_function_shape='ovr')),])\n",
    "\n",
    "\n",
    "# print(simple_split(svm_svd, 'Linear SVM with SVD'))\n",
    "svm_svd_res = ten_fold_cross_validation_metrics(svm_svd,'Linear SVM with SVD')\n",
    "\n",
    "EvaluationMetric_10fold.append([svm_svd_res[0][0], svm_svd_res[0][2], svm_svd_res[0][1], svm_svd_res[0][3]])\n",
    "\n",
    "print(\"Linear SVM with SVD:\", \"\\n Accuracy: \", svm_svd_res[0][0], \"\\n Recall: \", svm_svd_res[0][1], \"\\n Precision: \", svm_svd_res[0][2], \"\\n F1-score: \", svm_svd_res[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest__ using __SVD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Random Forest with SVD: \n",
      " Accuracy:  0.8663811412893793 \n",
      " Recall:  0.8389792489473311 \n",
      " Precision:  0.8795860019576922 \n",
      " F1-score:  0.8490384492737124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "random_forest_svd = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('svd', TruncatedSVD(n_components=component_num)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=240, max_depth=30, random_state=0)),])\n",
    "\n",
    "\n",
    "# print(simple_split(random_forest_bow, 'Random Forest with SVD'))\n",
    "\n",
    "random_forest_svd_res = ten_fold_cross_validation_metrics(random_forest_svd,'Random Forest with SVD')\n",
    "\n",
    "EvaluationMetric_10fold.append([random_forest_svd_res[0][0], random_forest_svd_res[0][2], random_forest_svd_res[0][1], random_forest_svd_res[0][3]])\n",
    "print(\"Random Forest with SVD:\", \"\\n Accuracy: \", random_forest_svd_res[0][0], \"\\n Recall: \", random_forest_svd_res[0][1], \"\\n Precision: \", random_forest_svd_res[0][2], \"\\n F1-score: \", random_forest_svd_res[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__W2V (Doc -> Sentence -> Word) TESTING**__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 379003 sentences in our corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/PycharmProjects/data_mining/venv/lib/python3.6/site-packages/ipykernel_launcher.py:59: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "/home/nik/PycharmProjects/data_mining/venv/lib/python3.6/site-packages/ipykernel_launcher.py:62: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('videos', 0.7882595062255859),\n",
       " ('channels', 0.7623605728149414),\n",
       " ('spotify', 0.7340512871742249),\n",
       " ('facebook', 0.7319403886795044),\n",
       " ('video', 0.7314403057098389),\n",
       " ('minecraft', 0.7295858263969421),\n",
       " ('content', 0.7102934718132019),\n",
       " ('ads', 0.7091004848480225),\n",
       " ('twitch', 0.7027051448822021),\n",
       " ('adverts', 0.6964372992515564)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "import re\n",
    "# nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "from gensim.models import Word2Vec\n",
    "# from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "\n",
    "\n",
    "def sentence_to_wordlist(sentence, remove_stopwords=False):\n",
    "    \n",
    "    sentence_text = re.sub(r'[^\\w\\s]','', sentence) # Remove non-letters\n",
    "    words = sentence_text.lower().split()  # Convert words to lower case and split them\n",
    "    return(words) # Return a list of words\n",
    "\n",
    "\n",
    "def doc_to_sentences(doc, tokenizer, remove_stopwords=False ):\n",
    "    try:\n",
    "        \n",
    "        raw_sentences = tokenizer.tokenize(doc.strip()) # NLTK tokenizer: split the text into sentences\n",
    "\n",
    "        sentences = [] \n",
    "        for raw_sentence in raw_sentences:         # Loop over each sentence\n",
    "            \n",
    "            if len(raw_sentence) > 0: # sentence_to_wordlist to get a list of words\n",
    "                sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "                \n",
    "        len(sentences)  # Return the list of sentences (each sentence is a list of words, so this returns a list of lists)\n",
    "        return sentences\n",
    "    except:\n",
    "        print('nope')\n",
    "\n",
    "\n",
    "doc_list = train_df['Content'].tolist()\n",
    "sentences = []\n",
    "\n",
    "for i in range(0,len(doc_list)):\n",
    "    try:\n",
    "        # Need to first change \"./.\" to \".\" so that sentences parse correctly\n",
    "        # oped = doc_list[i].replace(\"/.\", '')\n",
    "        # Now apply functions\n",
    "        sentences += doc_to_sentences(doc_list[i], tokenizer)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "        \n",
    "print(\"There are \" + str(len(sentences)) + \" sentences in our corpus.\")\n",
    "\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 50   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "w2v_model = Word2Vec(sentences, workers=num_workers,size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "w2v = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))\n",
    "\n",
    "\n",
    "w2v_model.most_similar('youtube',  topn=10)\n",
    "\n",
    "# w2v_model['google']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__W2V Functions: Vectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/PycharmProjects/data_mining/venv/lib/python3.6/site-packages/ipykernel_launcher.py:64: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.items())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(next(iter(word2vec.items())))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = cl.defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "    \n",
    "num_features = 400      # Word vector dimensionality     300                 \n",
    "min_word_count = 140  # Minimum word count            300            \n",
    "num_workers = 4        # Number of threads to run in parallel\n",
    "context = 10            # Context window size                       18                                                              \n",
    "downsampling = 1e-3    # Downsample setting for frequent words\n",
    "\n",
    "# train_df['tokenized_sents'] = train_df.apply(lambda row: nltk.word_tokenize(row['Content']), axis=1)\n",
    "# print('Dataframe Content Tokenized')\n",
    "# train_df['tokenized_content'] = train_df.apply(lambda row: re.sub(r'[^\\w\\s]','', row['Content']).lower(), axis=1)\n",
    "\n",
    "model = Word2Vec(train_df['Content'] , \n",
    "                 workers=num_workers,size=num_features, min_count = min_word_count, \n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear SVM__ using __W2V__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "SVM with W2V and TF-idf: \n",
      " Accuracy:  0.7149016969979449 \n",
      " Recall:  0.6748310471970421 \n",
      " Precision:  0.7318328257148756 \n",
      " F1-score:  0.6763408779272104\n"
     ]
    }
   ],
   "source": [
    "w2v_svm = Pipeline([\n",
    "    # (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    ('svm', svm.SVC(kernel='linear', C=1.0, decision_function_shape='ovr')),])\n",
    "    # (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "# train_df['tokenized_content'] = train_df.apply(lambda row: re.sub(r'[^\\w\\s]','', row['Content'].lower()), axis=1)\n",
    "# print(simple_split(w2v_svm, 'SVM with W2V'))\n",
    "w2v_svm_res = ten_fold_cross_validation_metrics(w2v_svm,'SVM with W2V and TF-idf')\n",
    "EvaluationMetric_10fold.append([w2v_svm_res[0][0], w2v_svm_res[0][2], w2v_svm_res[0][1], w2v_svm_res[0][3]])\n",
    "\n",
    "print(\"SVM with W2V and TF-idf:\", \"\\n Accuracy: \", w2v_svm_res[0][0], \"\\n Recall: \", w2v_svm_res[0][1], \"\\n Precision: \", w2v_svm_res[0][2], \"\\n F1-score: \", w2v_svm_res[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest__ using __W2V__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Random Forest with W2V and TF-idf: \n",
      " Accuracy:  0.6989226923158407 \n",
      " Recall:  0.6639647747872309 \n",
      " Precision:  0.7086530108451397 \n",
      " F1-score:  0.6685059562774113\n"
     ]
    }
   ],
   "source": [
    "w2v_rf = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=240, max_depth=30, random_state=0)),])\n",
    "\n",
    "#print(simple_split(w2v_rf, 'Random Forest with W2V'))\n",
    "w2v_rf_res = ten_fold_cross_validation_metrics(w2v_rf,'Random Forest with W2V and TF-idf')\n",
    "EvaluationMetric_10fold.append([w2v_rf_res[0][0], w2v_rf_res[0][2], w2v_rf_res[0][1], w2v_rf_res[0][3]])\n",
    "\n",
    "print(\"Random Forest with W2V and TF-idf:\", \"\\n Accuracy: \", w2v_rf_res[0][0], \"\\n Recall: \", w2v_rf_res[0][1], \"\\n Precision: \", w2v_rf_res[0][2], \"\\n F1-score: \", w2v_rf_res[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Beat the Benchmark__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Beat the BenchMark: \n",
      " Accuracy:  0.9698380330925191 \n",
      " Recall:  0.9683197665099137 \n",
      " Precision:  0.9678446084648099 \n",
      " F1-score:  0.9679768824057122\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Linear SVM with Stohastic Gradient Decent with Bag of Words and Tf-Idf transformation on features.\n",
    "\"\"\"\n",
    "# train_df['tokenized_content'] = train_df.apply(lambda row: row['Content'].lower(), axis=1)\n",
    "\n",
    "\n",
    "text_clf_svm = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='word', stop_words='english', lowercase=True)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', svm.SVC(kernel='linear', C=0.92, decision_function_shape='ovr')),])\n",
    "\n",
    "\n",
    "text_clf_svm_res = ten_fold_cross_validation_metrics(text_clf_svm,'Beat the BenchMark')\n",
    "EvaluationMetric_10fold.append([text_clf_svm_res[0][0], text_clf_svm_res[0][2], text_clf_svm_res[0][1], text_clf_svm_res[0][3]])\n",
    "\n",
    "print(\"Beat the BenchMark:\", \"\\n Accuracy: \", text_clf_svm_res[0][0], \"\\n Recall: \", text_clf_svm_res[0][1], \"\\n Precision: \", text_clf_svm_res[0][2], \"\\n F1-score: \", text_clf_svm_res[0][3])\n",
    "\n",
    "# res = ten_fold_cross_validation(text_clf_svm, 'Beat The BenchMark')\n",
    "# simple_split(text_clf_svm, 'Beat The BenchMark')\n",
    "# print(res) 0.9704084128148691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"APPEND RESULTS OF THE REQUESTED MODELS TO CSV\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    os.remove(EvaluationMetric_10fold_csv) # remove if exists\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "\n",
    "with open(EvaluationMetric_10fold_csv, mode='w') as csv_file:\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=['Statistic Measure', 'SVM (BOW)', 'Random Forest (BOW)','SVM (SVD)','Random Forest (SVD)','SVM (W2V)','Random Forest (W2V)', 'My method'], delimiter='\\t')\n",
    "    writer.writeheader()  \n",
    "    writer.writerow({'Statistic Measure': 'Accuracy', 'SVM (BOW)': EvaluationMetric_10fold[0][0], 'Random Forest (BOW)': EvaluationMetric_10fold[1][0],'SVM (SVD)': EvaluationMetric_10fold[2][0],'Random Forest (SVD)': EvaluationMetric_10fold[3][0],'SVM (W2V)': EvaluationMetric_10fold[4][0],'Random Forest (W2V)': EvaluationMetric_10fold[5][0], 'My method': EvaluationMetric_10fold[6][0]})\n",
    "    writer.writerow({'Statistic Measure': 'Precision', 'SVM (BOW)': EvaluationMetric_10fold[0][1], 'Random Forest (BOW)': EvaluationMetric_10fold[1][1],'SVM (SVD)': EvaluationMetric_10fold[2][1],'Random Forest (SVD)': EvaluationMetric_10fold[3][1],'SVM (W2V)': EvaluationMetric_10fold[4][1],'Random Forest (W2V)': EvaluationMetric_10fold[5][1], 'My method': EvaluationMetric_10fold[6][1]})\n",
    "    writer.writerow({'Statistic Measure': 'Recall', 'SVM (BOW)': EvaluationMetric_10fold[0][2], 'Random Forest (BOW)': EvaluationMetric_10fold[1][2],'SVM (SVD)': EvaluationMetric_10fold[2][2],'Random Forest (SVD)': EvaluationMetric_10fold[3][2],'SVM (W2V)': EvaluationMetric_10fold[4][2],'Random Forest (W2V)': EvaluationMetric_10fold[5][2], 'My method': EvaluationMetric_10fold[6][2]})\n",
    "    writer.writerow({'Statistic Measure': 'F-Measure', 'SVM (BOW)': EvaluationMetric_10fold[0][3], 'Random Forest (BOW)': EvaluationMetric_10fold[1][3],'SVM (SVD)': EvaluationMetric_10fold[2][3],'Random Forest (SVD)': EvaluationMetric_10fold[3][3],'SVM (W2V)': EvaluationMetric_10fold[4][3],'Random Forest (W2V)': EvaluationMetric_10fold[5][3], 'My method': EvaluationMetric_10fold[6][3]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Other experimental ways__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes__ with __BoW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for Naive Bayes with BoW is 0.9430178527757398\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('naive_bayes', MultinomialNB(fit_prior=False)),])\n",
    "\n",
    "\n",
    "print(simple_split(nb, 'Naive Bayes with BoW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for KNN with BoW is 0.9505991685008559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\"\"\"\n",
    "K Neighrest Neighbors\n",
    "\n",
    "number of neighbors: 13\n",
    "\"\"\"\n",
    "knn = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=13)),])\n",
    "\n",
    "print(simple_split(knn, 'KNN with BoW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for Neural Network - MLP with BoW is 0.9662509170946442\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\"\"\"\n",
    "MLP Classifier\n",
    "\n",
    "with Stohastic Gradient Decent\n",
    "RELU Activation function\n",
    "\n",
    "alpha: L2 penalty\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "lr_ = 0.0003 # learning rate\n",
    "l2_ = 0.0001 # l2 penalty\n",
    "\n",
    "mlp = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=(50,50,50 ), activation='relu', solver='adam', alpha=l2_, batch_size='auto', learning_rate='constant', learning_rate_init=lr_, max_iter=150)),])\n",
    "\n",
    "print(simple_split(mlp, 'Neural Network - MLP with BoW'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quadratic Discriminant Analysis using SVD__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for Quadratic Discriminant Analysis with BoW is 0.9557348985081927\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Needs SVD because Matrix is too sparse\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components=component_num)),\n",
    "    ('qda', QuadraticDiscriminantAnalysis()),])\n",
    "\n",
    "print(simple_split(qda, 'Quadratic Discriminant Analysis with BoW'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes with Gaussian Multinomial Distribution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for Gaussian Naive Bayes is 0.8625580826607973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svd', TruncatedSVD(n_components=component_num)),\n",
    "    ('g_naive_bayes', GaussianNB()),])\n",
    "\n",
    "print(simple_split(nb, 'Gaussian Naive Bayes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for AdaBoostClassifier with BoW is 0.8725849841036928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('abc', AdaBoostClassifier(n_estimators=240)),])\n",
    "\n",
    "print(simple_split(abc, 'AdaBoostClassifier with BoW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<split> Accuracy Score for Naive Bayes with BoW is 0.9684519442406456\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "    \n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                    ('mnb', MultinomialNB(fit_prior=False)),])\n",
    "\n",
    "print(simple_split(text_mnb_stemmed, 'Naive Bayes with BoW'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SVM with Stohastic Gradient Descent__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10fcv - Iteration 1\n",
      "10fcv - Iteration 2\n",
      "10fcv - Iteration 3\n",
      "10fcv - Iteration 4\n",
      "10fcv - Iteration 5\n",
      "10fcv - Iteration 6\n",
      "10fcv - Iteration 7\n",
      "10fcv - Iteration 8\n",
      "10fcv - Iteration 9\n",
      "10fcv - Iteration 10\n",
      "Beat the BenchMark: \n",
      " Accuracy:  0.9700831285354126 \n",
      " Recall:  0.9681692044135328 \n",
      " Precision:  0.967944849932891 \n",
      " F1-score:  0.9679714069688066\n"
     ]
    }
   ],
   "source": [
    "_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-4, max_iter=120, \n",
    "                                                   tol=1e-3,random_state=42, n_iter_no_change= 40)),])\n",
    "\n",
    "# print(simple_split(_clf_svm, 'Test'))\n",
    "aaa = ten_fold_cross_validation_metrics(_clf_svm,'SVM with Gradient Decent')\n",
    "print(\"SVM with Gradient Decent:\", \"\\n Accuracy: \", aaa[0][0], \"\\n Recall: \", aaa[0][1], \"\\n Precision: \", aaa[0][2], \"\\n F1-score: \", aaa[0][3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
